{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T18:59:42.591407Z",
     "start_time": "2024-12-27T18:59:38.849970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import string\n",
    "import modin.pandas as pd\n",
    "import pyap\n",
    "import nltk\n",
    "import emoji\n",
    "import re\n",
    "import tabulate\n",
    "import statistics\n",
    "import textstat\n",
    "import time\n",
    "# from tqdm import tqdm\n",
    "# from modin.config import ProgressBar\n",
    "# ProgressBar.enable()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "df = pd.read_csv(\"data/train.csv\", encoding=\"utf-8\")\n",
    "df.columns = ['0', \"id\", \"datetime\", \"query\", \"username\", \"text\"]\n",
    "\n",
    "cleaned_df = df.copy()\n",
    "stats_column = [\"WordCount\", \"SentenceCount\", \"AvgSentenceLen\", \"MaxSentenceLen\", \"MinSentenceLen\", \"MaxWordLen\",\n",
    "                \"Emoticons\", \"StopWords\", \"NumLowerCase\", \"NumSpecialChars\", \"Address\", \"PhoneNum\", \"AccountNum\"]"
   ],
   "id": "fb3a34bd63da0a67",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/khantzawhein/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/khantzawhein/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/khantzawhein/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "2024-12-28 01:59:41,130\tINFO worker.py:1821 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T18:59:42.597327Z",
     "start_time": "2024-12-27T18:59:42.594555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = nltk.TweetTokenizer()\n",
    "\n",
    "def find_addresses(text):\n",
    "    return [address for address in pyap.parse(text, country='US')]\n",
    "\n",
    "\n",
    "def find_emoticons_and_emojis(text):\n",
    "    emojis = [i['emoji'] for i in emoji.emoji_list(text)]\n",
    "    emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    return emojis + emoticons\n",
    "\n",
    "\n",
    "def find_phone_numbers(text):\n",
    "    return re.findall(r'^[\\+]?[(]?[0-9]{3}[)]?[-\\s\\.]?[0-9]{3}[-\\s\\.]?[0-9]{4,6}$', text)\n",
    "\n",
    "\n",
    "def find_account_numbers(text):\n",
    "    return re.findall(r'\\b\\d{9,18}\\b', text)\n",
    "\n",
    "\n",
    "def find_special_chars(text):\n",
    "    return re.findall(r'[^\\w\\s]', text)\n",
    "\n",
    "\n",
    "def find_lowercase(text):\n",
    "    return re.findall(r'[a-z]', text)\n",
    "\n",
    "\n",
    "def find_stop_words(text):\n",
    "    return [char for char in text if char in stopwords]\n",
    "\n",
    "\n",
    "def word_tokenize(text):\n",
    "    # Tweet mentions and hashtags aware tokenizer\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "\n",
    "def sentence_tokenize(text):\n",
    "    return nltk.sent_tokenize(text)\n"
   ],
   "id": "21c63d43325b0b9f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T19:00:05.959580Z",
     "start_time": "2024-12-27T18:59:42.741430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "before_stat_start_time = time.time()\n",
    "\n",
    "before_stats_for_rows = pd.DataFrame(columns=stats_column)\n",
    "before_sentences = df[\"text\"].apply(lambda x: sentence_tokenize(x))\n",
    "before_words = df[\"text\"].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "before_stats_for_rows[\"WordCount\"] = before_words.apply(len)\n",
    "before_stats_for_rows[\"SentenceCount\"] = before_sentences.apply(len)\n",
    "before_stats_for_rows[\"AvgSentenceLen\"] = before_sentences.apply(\n",
    "    lambda x: statistics.mean([len(sentence) for sentence in x]))\n",
    "before_stats_for_rows[\"MaxSentenceLen\"] = before_sentences.apply(lambda x: max([len(sentence) for sentence in x]))\n",
    "before_stats_for_rows[\"MinSentenceLen\"] = before_sentences.apply(lambda x: min([len(sentence) for sentence in x]))\n",
    "before_stats_for_rows[\"MaxWordLen\"] = before_words.apply(lambda x: max([len(word) for word in x]))\n",
    "before_stats_for_rows[\"Emoticons\"] = df[\"text\"].apply(lambda x: len(find_emoticons_and_emojis(x)))\n",
    "before_stats_for_rows[\"StopWords\"] = df[\"text\"].apply(lambda x: len(find_stop_words(x)))\n",
    "before_stats_for_rows[\"NumLowerCase\"] = df[\"text\"].apply(lambda x: len(find_lowercase(x)))\n",
    "before_stats_for_rows[\"NumSpecialChars\"] = df[\"text\"].apply(lambda x: len(find_special_chars(x)))\n",
    "before_stats_for_rows[\"Address\"] = df[\"text\"].apply(lambda x: len(find_addresses(x)))\n",
    "before_stats_for_rows[\"PhoneNum\"] = df[\"text\"].apply(lambda x: len(find_phone_numbers(x)))\n",
    "before_stats_for_rows[\"AccountNum\"] = df[\"text\"].apply(lambda x: len(find_account_numbers(x)))\n",
    "\n",
    "print(before_stats_for_rows.head(5))\n",
    "\n",
    "aggregated_before_stats = before_stats_for_rows.agg(['sum', 'mean', 'max', 'min'])\n",
    "print(aggregated_before_stats)\n",
    "\n",
    "before_stat_end_time = time.time()\n"
   ],
   "id": "587a2d8c8ddc94e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   WordCount  SentenceCount  AvgSentenceLen  MaxSentenceLen  MinSentenceLen  \\\n",
      "0         22              2           55.00             105               5   \n",
      "1         18              2           44.00              46              42   \n",
      "2         10              1           46.00              46              46   \n",
      "3         21              4           26.75              46               8   \n",
      "4          5              1           28.00              28              28   \n",
      "\n",
      "   MaxWordLen  Emoticons  StopWords  NumLowerCase  NumSpecialChars  Address  \\\n",
      "0           8          0         45            81                6        0   \n",
      "1           9          0         31            62                3        0   \n",
      "2           5          0         18            37                0        0   \n",
      "3          16          0         39            80                9        0   \n",
      "4           9          0          8            22                1        0   \n",
      "\n",
      "   PhoneNum  AccountNum  \n",
      "0         0           0  \n",
      "1         0           0  \n",
      "2         0           0  \n",
      "3         0           0  \n",
      "4         0           0  \n",
      "         WordCount  SentenceCount  AvgSentenceLen  MaxSentenceLen  \\\n",
      "sum   2.145804e+07   2.747515e+06    7.749788e+07    9.142833e+07   \n",
      "mean  1.341128e+01   1.717198e+00    4.843620e+01    5.714274e+01   \n",
      "max   2.280000e+02   6.400000e+01    3.590000e+02    3.590000e+02   \n",
      "min   1.000000e+00   1.000000e+00    1.140625e+00    3.000000e+00   \n",
      "\n",
      "      MinSentenceLen    MaxWordLen   Emoticons     StopWords  NumLowerCase  \\\n",
      "sum     6.463208e+07  1.667643e+07  16320.0000  4.116132e+07  8.458836e+07   \n",
      "mean    4.039507e+01  1.042277e+01      0.0102  2.572584e+01  5.286776e+01   \n",
      "max     3.590000e+02  1.240000e+02     79.0000  1.170000e+02  1.470000e+02   \n",
      "min     1.000000e+00  2.000000e+00      0.0000  0.000000e+00  0.000000e+00   \n",
      "\n",
      "      NumSpecialChars     Address  PhoneNum  AccountNum  \n",
      "sum      6.309898e+06  731.000000       0.0  682.000000  \n",
      "mean     3.943689e+00    0.000457       0.0    0.000426  \n",
      "max      3.330000e+02    2.000000       0.0    2.000000  \n",
      "min      0.000000e+00    0.000000       0.0    0.000000  \n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Cleaning",
   "id": "1a43eed27872a68a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T19:00:06.396955Z",
     "start_time": "2024-12-27T19:00:06.394318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_stop_words(text):\n",
    "    return \" \".join([word for word in text.split() if word not in stopwords])\n",
    "\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return \"\".join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "\n",
    "def lemmaize_tokens(tokens):\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word, pos=\"v\") for word in tokens]\n",
    "\n",
    "\n",
    "def remove_non_english_words(tokens):\n",
    "    return [token for token in tokens if token in words]\n",
    "\n",
    "\n",
    "def remove_emoticons(text):\n",
    "    text = re.sub(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', '', text)\n",
    "    return emoji.replace_emoji(text, \"\")\n",
    "\n",
    "\n",
    "def remove_phone_numbers(text):\n",
    "    return re.sub(r'^[\\+]?[(]?[0-9]{3}[)]?[-\\s\\.]?[0-9]{3}[-\\s\\.]?[0-9]{4,6}$', '', text)\n",
    "\n",
    "\n",
    "def remove_account_numbers(text):\n",
    "    return re.sub(r'\\b\\d{9,18}\\b', '', text)\n"
   ],
   "id": "eb0b9342d5e32e51",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T19:00:06.873409Z",
     "start_time": "2024-12-27T19:00:06.871187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    # pipeline for cleaning text\n",
    "    text = remove_stop_words(text)\n",
    "    text = remove_special_chars(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_emoticons(text)\n",
    "    text = remove_phone_numbers(text)\n",
    "    text = remove_account_numbers(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = remove_non_english_words(tokens)\n",
    "    tokens = lemmaize_tokens(tokens)\n",
    "    text = \" \".join(tokens).strip()\n",
    "\n",
    "    return text if text else None\n"
   ],
   "id": "b4da1e239a58cf7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T19:00:19.346803Z",
     "start_time": "2024-12-27T19:00:07.386826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cleaning_start_time = time.time()\n",
    "\n",
    "\n",
    "cleaned_df[\"text\"] = df[\"text\"].apply(lambda x: clean_text(x))\n",
    "\n",
    "cleaned_df.dropna(subset=[\"text\"], inplace=True)\n",
    "\n",
    "# Clean text using NLTK:\n",
    "\n",
    "print(cleaned_df.head(5))\n",
    "\n",
    "cleaning_end_time = time.time()"
   ],
   "id": "74ba442f265c8507",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0          id                      datetime     query       username  \\\n",
      "0  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY  scotthamilton   \n",
      "1  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY       mattycus   \n",
      "2  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY        ElleCTF   \n",
      "3  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         Karoli   \n",
      "4  0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY       joy_wolf   \n",
      "\n",
      "                                                text  \n",
      "0  upset cant update it might cry result school t...  \n",
      "1                        many time ball save rest go  \n",
      "2                         whole body itchy like fire  \n",
      "3                     no all mad here cant see there  \n",
      "4                                         whole crew  \n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T19:00:22.164106Z",
     "start_time": "2024-12-27T19:00:20.142365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"data/cleaned_train.csv\", \"w\") as file:\n",
    "    cleaned_df.to_csv(file, index=False)"
   ],
   "id": "637c6e252283c408",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: `to_csv` is not currently supported by PandasOnRay, defaulting to pandas implementation.\n",
      "Please refer to https://modin.readthedocs.io/en/stable/supported_apis/defaulting_to_pandas.html for explanation.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T19:00:36.024304Z",
     "start_time": "2024-12-27T19:00:23.321676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#After Stats\n",
    "after_stats_start_time = time.time()\n",
    "\n",
    "after_stats_for_rows = pd.DataFrame(columns=stats_column)\n",
    "after_sentences = cleaned_df[\"text\"].apply(lambda x: sentence_tokenize(x))\n",
    "after_words = cleaned_df[\"text\"].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "after_stats_for_rows[\"WordCount\"] = after_words.apply(len)\n",
    "after_stats_for_rows[\"SentenceCount\"] = after_sentences.apply(len)\n",
    "after_stats_for_rows[\"AvgSentenceLen\"] = after_sentences.apply(\n",
    "    lambda x: statistics.mean([len(sentence) for sentence in x]) if x else 0)\n",
    "after_stats_for_rows[\"MaxSentenceLen\"] = after_sentences.apply(\n",
    "    lambda x: max([len(sentence) for sentence in x]) if x else 0)\n",
    "after_stats_for_rows[\"MinSentenceLen\"] = after_sentences.apply(\n",
    "    lambda x: min([len(sentence) for sentence in x]) if x else 0)\n",
    "after_stats_for_rows[\"MaxWordLen\"] = after_words.apply(lambda x: max([len(word) for word in x]) if x else 0)\n",
    "after_stats_for_rows[\"Emoticons\"] = cleaned_df[\"text\"].apply(lambda x: len(find_emoticons_and_emojis(x)))\n",
    "after_stats_for_rows[\"StopWords\"] = cleaned_df[\"text\"].apply(lambda x: len(find_stop_words(x)))\n",
    "after_stats_for_rows[\"NumLowerCase\"] = cleaned_df[\"text\"].apply(lambda x: len(find_lowercase(x)))\n",
    "after_stats_for_rows[\"NumSpecialChars\"] = cleaned_df[\"text\"].apply(lambda x: len(find_special_chars(x)))\n",
    "after_stats_for_rows[\"Address\"] = cleaned_df[\"text\"].apply(lambda x: len(find_addresses(x)))\n",
    "after_stats_for_rows[\"PhoneNum\"] = cleaned_df[\"text\"].apply(lambda x: len(find_phone_numbers(x)))\n",
    "after_stats_for_rows[\"AccountNum\"] = cleaned_df[\"text\"].apply(lambda x: len(find_account_numbers(x)))\n",
    "\n",
    "print(after_stats_for_rows.head(5))\n",
    "\n",
    "aggregated_after_stats = after_stats_for_rows.agg(['sum', 'mean', 'max', 'min'])\n",
    "print(aggregated_after_stats)\n",
    "\n",
    "after_stats_end_time = time.time()\n"
   ],
   "id": "413d9d1b5155b866",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   WordCount  SentenceCount  AvgSentenceLen  MaxSentenceLen  MinSentenceLen  \\\n",
      "0         11              1              60              60              60   \n",
      "1          6              1              27              27              27   \n",
      "2          5              1              26              26              26   \n",
      "3          7              1              30              30              30   \n",
      "4          2              1              10              10              10   \n",
      "\n",
      "   MaxWordLen  Emoticons  StopWords  NumLowerCase  NumSpecialChars  Address  \\\n",
      "0           6          0         27            50                0        0   \n",
      "1           4          0         12            22                0        0   \n",
      "2           5          0          9            22                0        0   \n",
      "3           5          0          9            24                0        0   \n",
      "4           5          0          1             9                0        0   \n",
      "\n",
      "   PhoneNum  AccountNum  \n",
      "0         0           0  \n",
      "1         0           0  \n",
      "2         0           0  \n",
      "3         0           0  \n",
      "4         0           0  \n",
      "         WordCount  SentenceCount  AvgSentenceLen  MaxSentenceLen  \\\n",
      "sum   8.414769e+06      1562107.0    4.538922e+07    4.538922e+07   \n",
      "mean  5.386807e+00            1.0    2.905641e+01    2.905641e+01   \n",
      "max   3.300000e+01            1.0    1.280000e+02    1.280000e+02   \n",
      "min   1.000000e+00            1.0    1.000000e+00    1.000000e+00   \n",
      "\n",
      "      MinSentenceLen    MaxWordLen  Emoticons     StopWords  NumLowerCase  \\\n",
      "sum     4.538922e+07  1.026731e+07        0.0  1.718609e+07  3.853656e+07   \n",
      "mean    2.905641e+01  6.572729e+00        0.0  1.100187e+01  2.466960e+01   \n",
      "max     1.280000e+02  2.200000e+01        0.0  7.100000e+01  1.070000e+02   \n",
      "min     1.000000e+00  1.000000e+00        0.0  0.000000e+00  1.000000e+00   \n",
      "\n",
      "      NumSpecialChars   Address  PhoneNum  AccountNum  \n",
      "sum               0.0  3.000000       0.0         0.0  \n",
      "mean              0.0  0.000002       0.0         0.0  \n",
      "max               0.0  1.000000       0.0         0.0  \n",
      "min               0.0  0.000000       0.0         0.0  \n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T19:00:44.133991Z",
     "start_time": "2024-12-27T19:00:36.660129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compare before and after stats\n",
    "header = [\"Stats\", \"Before\", \"After\", \"Diff\"]\n",
    "before_vocab_size = len(set([word for words in before_words for word in words]))\n",
    "after_vocab_size = len(set([word for words in after_words for word in words]))\n",
    "\n",
    "table = [\n",
    "    [\"Avg Sentence Length\", aggregated_before_stats[\"AvgSentenceLen\"][\"mean\"],\n",
    "     aggregated_after_stats[\"AvgSentenceLen\"][\"mean\"],\n",
    "     aggregated_after_stats[\"AvgSentenceLen\"][\"mean\"] - aggregated_before_stats[\"AvgSentenceLen\"][\"mean\"]],\n",
    "    [\"Max Sentence Length\", aggregated_before_stats[\"MaxSentenceLen\"][\"max\"],\n",
    "     aggregated_after_stats[\"MaxSentenceLen\"][\"max\"],\n",
    "     aggregated_after_stats[\"MaxSentenceLen\"][\"max\"] - aggregated_before_stats[\"MaxSentenceLen\"][\"max\"]],\n",
    "    [\"Min Sentence Length\", aggregated_before_stats[\"MinSentenceLen\"][\"min\"],\n",
    "     aggregated_after_stats[\"MinSentenceLen\"][\"min\"],\n",
    "     aggregated_after_stats[\"MinSentenceLen\"][\"min\"] - aggregated_before_stats[\"MinSentenceLen\"][\"min\"]],\n",
    "    [\"Max Word Length\", aggregated_before_stats[\"MaxWordLen\"][\"max\"], aggregated_after_stats[\"MaxWordLen\"][\"max\"],\n",
    "     aggregated_after_stats[\"MaxWordLen\"][\"max\"] - aggregated_before_stats[\"MaxWordLen\"][\"max\"]],\n",
    "    [\"Word Count\", aggregated_before_stats[\"WordCount\"][\"sum\"], aggregated_after_stats[\"WordCount\"][\"sum\"],\n",
    "     aggregated_after_stats[\"WordCount\"][\"sum\"] - aggregated_before_stats[\"WordCount\"][\"sum\"]],\n",
    "    [\"Sentence Count\", aggregated_before_stats[\"SentenceCount\"][\"sum\"], aggregated_after_stats[\"SentenceCount\"][\"sum\"],\n",
    "     aggregated_after_stats[\"SentenceCount\"][\"sum\"] - aggregated_before_stats[\"SentenceCount\"][\"sum\"]],\n",
    "    [\"Vocab Size\", before_vocab_size, after_vocab_size, after_vocab_size - before_vocab_size],\n",
    "    [\"Emoticons\", aggregated_before_stats[\"Emoticons\"][\"sum\"], aggregated_after_stats[\"Emoticons\"][\"sum\"],\n",
    "     aggregated_after_stats[\"Emoticons\"][\"sum\"] - aggregated_before_stats[\"Emoticons\"][\"sum\"]],\n",
    "    [\"Stop Words\", aggregated_before_stats[\"StopWords\"][\"sum\"], aggregated_after_stats[\"StopWords\"][\"sum\"],\n",
    "     aggregated_after_stats[\"StopWords\"][\"sum\"] - aggregated_before_stats[\"StopWords\"][\"sum\"]],\n",
    "    [\"Num Lower Case\", aggregated_before_stats[\"NumLowerCase\"][\"sum\"], aggregated_after_stats[\"NumLowerCase\"][\"sum\"],\n",
    "     aggregated_after_stats[\"NumLowerCase\"][\"sum\"] - aggregated_before_stats[\"NumLowerCase\"][\"sum\"]],\n",
    "    [\"Num Special Chars\", aggregated_before_stats[\"NumSpecialChars\"][\"sum\"],\n",
    "     aggregated_after_stats[\"NumSpecialChars\"][\"sum\"],\n",
    "     aggregated_after_stats[\"NumSpecialChars\"][\"sum\"] - aggregated_before_stats[\"NumSpecialChars\"][\"sum\"]],\n",
    "    [\"Address\", aggregated_before_stats[\"Address\"][\"sum\"], aggregated_after_stats[\"Address\"][\"sum\"],\n",
    "     aggregated_after_stats[\"Address\"][\"sum\"] - aggregated_before_stats[\"Address\"][\"sum\"]],\n",
    "    [\"Phone Num\", aggregated_before_stats[\"PhoneNum\"][\"sum\"], aggregated_after_stats[\"PhoneNum\"][\"sum\"],\n",
    "     aggregated_after_stats[\"PhoneNum\"][\"sum\"] - aggregated_before_stats[\"PhoneNum\"][\"sum\"]],\n",
    "    [\"Account Num\", aggregated_before_stats[\"AccountNum\"][\"sum\"], aggregated_after_stats[\"AccountNum\"][\"sum\"],\n",
    "     aggregated_after_stats[\"AccountNum\"][\"sum\"] - aggregated_before_stats[\"AccountNum\"][\"sum\"]]\n",
    "]\n",
    "print(\"Stats Comparison Before and After Cleaning\")\n",
    "print(tabulate.tabulate(table, headers=header, tablefmt=\"pretty\"))\n"
   ],
   "id": "2c3954b2250a49b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats Comparison Before and After Cleaning\n",
      "+---------------------+-------------------+-------------------+--------------------+\n",
      "|        Stats        |      Before       |       After       |        Diff        |\n",
      "+---------------------+-------------------+-------------------+--------------------+\n",
      "| Avg Sentence Length | 48.43620461628908 | 29.05640842784777 | -19.37979618844131 |\n",
      "| Max Sentence Length |       359.0       |       128.0       |       -231.0       |\n",
      "| Min Sentence Length |        1.0        |        1.0        |        0.0         |\n",
      "|   Max Word Length   |       124.0       |       22.0        |       -102.0       |\n",
      "|     Word Count      |    21458036.0     |     8414769.0     |    -13043267.0     |\n",
      "|   Sentence Count    |     2747515.0     |     1562107.0     |     -1185408.0     |\n",
      "|     Vocab Size      |      859393       |       29412       |      -829981       |\n",
      "|      Emoticons      |      16320.0      |        0.0        |      -16320.0      |\n",
      "|     Stop Words      |    41161316.0     |    17186092.0     |    -23975224.0     |\n",
      "|   Num Lower Case    |    84588360.0     |    38536557.0     |    -46051803.0     |\n",
      "|  Num Special Chars  |     6309898.0     |        0.0        |     -6309898.0     |\n",
      "|       Address       |       731.0       |        3.0        |       -728.0       |\n",
      "|      Phone Num      |        0.0        |        0.0        |        0.0         |\n",
      "|     Account Num     |       682.0       |        0.0        |       -682.0       |\n",
      "+---------------------+-------------------+-------------------+--------------------+\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T19:00:45.802711Z",
     "start_time": "2024-12-27T19:00:44.143328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Readability Score\n",
    "\n",
    "readability_score = cleaned_df[\"text\"].apply(textstat.flesch_reading_ease).mean()\n",
    "\n",
    "print(f\"Readability Score: {readability_score}\")\n",
    "\n",
    "lexical_diversity = after_vocab_size / aggregated_after_stats[\"WordCount\"][\"sum\"]\n",
    "\n",
    "print(f\"Lexical Diversity: {lexical_diversity}\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken for stats before cleaning: {round((before_stat_end_time - before_stat_start_time), 2)} seconds\")\n",
    "print(f\"Time taken for cleaning: {round((cleaning_end_time - cleaning_start_time), 2)} seconds\")\n",
    "print(\n",
    "    f\"Time taken for stats after cleaning: {round((after_stats_end_time - after_stats_start_time), 2)} seconds\")\n",
    "print(f\"Total time taken: {round((end_time - start_time), 2)} seconds\")\n"
   ],
   "id": "7bc3b001519bf038",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readability Score: 88.6915177449432\n",
      "Lexical Diversity: 0.003495283114723648\n",
      "Time taken for stats before cleaning: 23.21 seconds\n",
      "Time taken for cleaning: 11.96 seconds\n",
      "Time taken for stats after cleaning: 12.7 seconds\n",
      "Total time taken: 66.2 seconds\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
